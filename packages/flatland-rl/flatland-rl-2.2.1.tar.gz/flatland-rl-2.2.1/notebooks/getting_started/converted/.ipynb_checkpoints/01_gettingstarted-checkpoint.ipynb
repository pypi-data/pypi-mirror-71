{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Started Tutorial\n",
    "========================\n",
    "\n",
    "Overview\n",
    "--------\n",
    "\n",
    "Following are three short tutorials to help new users get acquainted\n",
    "with how to create RailEnvs, how to train simple DQN agents on them, and\n",
    "how to customize them.\n",
    "\n",
    "To use flatland in a project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flatland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Example 1 : Basic Usage\n",
    "------------------------------\n",
    "\n",
    "The basic usage of RailEnv environments consists in creating a RailEnv\n",
    "object endowed with a rail generator, that generates new rail networks\n",
    "on each reset, and an observation generator object, that is supplied\n",
    "with environment-specific information at each time step and provides a\n",
    "suitable observation vector to the agents. After the RailEnv environment\n",
    "is created, one need to call reset() on the environment in order to\n",
    "fully initialize the environment\n",
    "\n",
    "The simplest rail generators are\n",
    "envs.rail\\_generators.rail\\_from\\_manual\\_specifications\\_generator and\n",
    "envs.rail\\_generators.random\\_rail\\_generator.\n",
    "\n",
    "The first one accepts a list of lists whose each element is a 2-tuple,\n",
    "whose entries represent the \\'cell\\_type\\' (see\n",
    "core.transitions.RailEnvTransitions) and the desired clockwise rotation\n",
    "of the cell contents (0, 90, 180 or 270 degrees). For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs = [[(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)],\n",
    "         [(0, 0), (0, 0), (0, 0), (0, 0), (7, 0), (0, 0)],\n",
    "         [(7, 270), (1, 90), (1, 90), (1, 90), (2, 90), (7, 90)],\n",
    "         [(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]]\n",
    "\n",
    "env = RailEnv(width=6,\n",
    "              height=4,\n",
    "              rail_generator=rail_from_manual_specifications_generator(specs),\n",
    "              number_of_agents=1,\n",
    "              obs_builder_object=TreeObsForRailEnv(max_depth=2))\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, a random environment can be generated (optionally\n",
    "specifying weights for each cell type to increase or decrease their\n",
    "proportion in the generated rail networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative weights of each cell type to be used by the random rail generators.\n",
    "transition_probability = [1.0,  # empty cell - Case 0\n",
    "                          1.0,  # Case 1 - straight\n",
    "                          1.0,  # Case 2 - simple switch\n",
    "                          0.3,  # Case 3 - diamond drossing\n",
    "                          0.5,  # Case 4 - single slip\n",
    "                          0.5,  # Case 5 - double slip\n",
    "                          0.2,  # Case 6 - symmetrical\n",
    "                          0.0,  # Case 7 - dead end\n",
    "                          0.2,  # Case 8 - turn left\n",
    "                          0.2,  # Case 9 - turn right\n",
    "                          1.0]  # Case 10 - mirrored switch\n",
    "\n",
    "# Example generate a random rail\n",
    "env = RailEnv(width=10,\n",
    "              height=10,\n",
    "              rail_generator=random_rail_generator(\n",
    "                        cell_type_relative_proportion=transition_probability\n",
    "                        ),\n",
    "              number_of_agents=3,\n",
    "              obs_builder_object=TreeObsForRailEnv(max_depth=2))\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments can be rendered using the utils.rendertools utilities, for\n",
    "example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_renderer = RenderTool(env)\n",
    "env_renderer.render_env(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the environment can be run by supplying the environment step\n",
    "function with a dictionary of actions whose keys are agents\\' handles\n",
    "(returned by env.get\\_agent\\_handles() ) and the corresponding values\n",
    "the selected actions. For example, for a 2-agents environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = env.get_agent_handles()\n",
    "action_dict = {handles[0]:0, handles[1]:0}\n",
    "obs, all_rewards, done, _ = env.step(action_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \\'obs\\', \\'all\\_rewards\\', and \\'done\\' are also dictionary\n",
    "indexed by the agents\\' handles, whose values correspond to the relevant\n",
    "observations, rewards and terminal status for each agent. Further, the\n",
    "\\'dones\\' dictionary returns an extra key \\'\\_\\_all\\_\\_\\' that is set to\n",
    "True after all agents have reached their goals.\n",
    "\n",
    "In the specific case a TreeObsForRailEnv observation builder is used, it\n",
    "is possible to print a representation of the returned observations with\n",
    "the following code. Also, tree observation data is displayed by\n",
    "RenderTool by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(env.get_num_agents()):\n",
    "    env.obs_builder.util_print_obs_subtree(\n",
    "            tree=obs[i],\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete code for this part of the Getting Started guide can be\n",
    "found in\n",
    "\n",
    "-   [examples/simple\\_example\\_1.py](https://gitlab.aicrowd.com/flatland/flatland/blob/master/examples/simple_example_1.py)\n",
    "-   [examples/simple\\_example\\_2.py](https://gitlab.aicrowd.com/flatland/flatland/blob/master/examples/simple_example_2.py)\n",
    "\n",
    "Part 2 : Training a Simple an Agent on Flatland\n",
    "-----------------------------------------------\n",
    "\n",
    "This is a brief tutorial on how to train an agent on Flatland. Here we\n",
    "use a simple random agent to illustrate the process on how to interact\n",
    "with the environment. The corresponding code can be found in\n",
    "examples/training\\_example.py and in the baselines repository you find a\n",
    "tutorial to train a [DQN](https://arxiv.org/abs/1312.5602) agent to\n",
    "solve the navigation task.\n",
    "\n",
    "We start by importing the necessary Flatland libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flatland.envs.rail_generators import complex_rail_generator\n",
    "from flatland.envs.schedule_generators import complex_schedule_generator\n",
    "from flatland.envs.rail_env import RailEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complex\\_rail\\_generator is used in order to guarantee feasible\n",
    "railway network configurations for training. Next we configure the\n",
    "difficulty of our task by modifying the complex\\_rail\\_generator\n",
    "parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RailEnv(  width=15,\n",
    "                height=15,\n",
    "                rail_generator=complex_rail_generator(\n",
    "                                    nr_start_goal=10,\n",
    "                                    nr_extra=10,\n",
    "                                    min_dist=10,\n",
    "                                    max_dist=99999,\n",
    "                                    seed=1),\n",
    "                number_of_agents=5)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficulty of a railway network depends on the dimensions\n",
    "([width]{.title-ref} x [height]{.title-ref}) and the number of agents in\n",
    "the network. By varying the number of start and goal connections\n",
    "(nr\\_start\\_goal) and the number of extra railway elements added\n",
    "(nr\\_extra) the number of alternative paths of each agents can be\n",
    "modified. The more possible paths an agent has to reach its target the\n",
    "easier the task becomes. Here we don\\'t specify any observation builder\n",
    "but rather use the standard tree observation. If you would like to use a\n",
    "custom obervation please follow the instructions in the next tutorial.\n",
    "Feel free to vary these parameters to see how your own agent holds up on\n",
    "different setting. The evalutation set of railway configurations will\n",
    "cover the whole spectrum from easy to complex tasks.\n",
    "\n",
    "Once we are set with the environment we can load our preferred agent\n",
    "from either RLlib or any other ressource. Here we use a random agent to\n",
    "illustrate the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RandomAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start every trial by resetting the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which provides the initial observation for all agents (obs = array of\n",
    "all observations). In order for the environment to step forward in time\n",
    "we need a dictionar of actions for all active agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for handle in range(env.get_num_agents()):\n",
    "    action = agent.act(obs[handle])\n",
    "    action_dict.update({handle: action})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary is then passed to the environment which checks the\n",
    "validity of all actions and update the environment state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_obs, all_rewards, done, _ = env.step(action_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment returns an array of new observations, reward dictionary\n",
    "for all agents as well as a flag for which agents are done. This\n",
    "information can be used to update the policy of your agent and if\n",
    "done\\[\\'\\_\\_all\\_\\_\\'\\] == True the episode terminates.\n",
    "\n",
    "The full source code of this example can be found in\n",
    "[examples/training\\_example.py](https://gitlab.aicrowd.com/flatland/flatland/blob/master/examples/training_example.py)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
