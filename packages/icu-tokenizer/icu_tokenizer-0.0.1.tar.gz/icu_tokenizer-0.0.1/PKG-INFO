Metadata-Version: 2.1
Name: icu_tokenizer
Version: 0.0.1
Summary: ICU based universal language tokenizer
Home-page: https://github.com/mingruimingrui/ICU-tokenizer
Author: Wang Ming Rui
Author-email: mingruimingrui@hotmail.com
License: MIT License
Description: **ICU-tokenizer** is a python package used to perform universal language
        normalization and tokenization using the International Components for
        Unicode.
        
        - [Install](#install)
        - [Usage (Python)](#usage-python)
          - [Sentence splitter](#sentence-splitter)
          - [Normalizer](#normalizer)
          - [Tokenizer](#tokenizer)
        
        ## Install
        
        See [./INSTALL.md](./INSTALL.md)
        
        ## Usage (Python)
        
        ### Sentence splitter
        
        ```py
        # To split a paragraph into multiple sentences
        >>> from icu_tokenizer import SentSplitter
        >>> splitter = SentSplitter('zh')
        
        >>> paragraph = """
        ÁæéÂõΩÊúÄÈ´òÊ≥ïÈô¢ÔºàËã±ËØ≠ÔºöSupreme Court of the United StatesÔºâÔºå‰∏ÄËà¨ÊòØÊåáÁæéÂõΩËÅîÈÇ¶ÊúÄÈ´òÊ≥ïÈô¢ÔºåÊòØÁæéÂõΩÊúÄÈ´òÁ∫ßÂà´ÁöÑËÅîÈÇ¶Ê≥ïÈô¢Ôºå‰∏∫ÁæéÂõΩ‰∏âÊùÉÁªßÊÄªÁªü„ÄÅÂõΩ‰ºöÂêéÊúÄ‰∏∫ÈáçË¶ÅÁöÑ‰∏ÄÁéØ„ÄÇÊ†πÊçÆ1789Âπ¥„ÄäÁæéÂõΩÂÆ™Ê≥ïÁ¨¨‰∏âÊù°„ÄãÁöÑËßÑÂÆöÔºåÊúÄÈ´òÊ≥ïÈô¢ÂØπÊâÄÊúâËÅîÈÇ¶Ê≥ïÈô¢„ÄÅÂ∑ûÊ≥ïÈô¢ÂíåÊ∂âÂèäËÅîÈÇ¶Ê≥ïÂæãÈóÆÈ¢òÁöÑËØâËÆºÊ°à‰ª∂ÂÖ∑ÊúâÊúÄÁªàÔºàÂπ∂‰∏îÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊòØÊúâÊñüÈÖåÂÜ≥ÂÆöÊùÉÁöÑÔºâ‰∏äËØâÁÆ°ËæñÊùÉÔºå‰ª•ÂèäÂØπÂ∞èËåÉÂõ¥Ê°à‰ª∂ÁöÑÂÖ∑ÊúâÂàùÂÆ°ÁÆ°ËæñÊùÉ„ÄÇÂú®ÁæéÂõΩÁöÑÊ≥ïÂæãÂà∂Â∫¶‰∏≠ÔºåÊúÄÈ´òÊ≥ïÈô¢ÈÄöÂ∏∏ÊòØÂåÖÊã¨„ÄäÁæéÂõΩÂÆ™Ê≥ï„ÄãÂú®ÂÜÖÁöÑËÅîÈÇ¶Ê≥ïÂæãÁöÑÊúÄÁªàËß£ÈáäËÄÖÔºå‰ΩÜ‰ªÖÂú®ÂÖ∑ÊúâÁÆ°ËæñÊùÉÁöÑÊ°à‰ª∂ËåÉÂõ¥ÂÜÖ„ÄÇÊ≥ïÈô¢‰∏ç‰∫´ÊúâÂà§ÂÆöÊîøÊ≤ªÈóÆÈ¢òÁöÑÊùÉÂäõÔºõÊîøÊ≤ªÈóÆÈ¢òÁöÑÊâßÊ≥ïÊú∫ÂÖ≥ÊòØË°åÊîøÊú∫ÂÖ≥ÔºåËÄå‰∏çÊòØÊîøÂ∫úÁöÑÂè∏Ê≥ïÈÉ®Èó®„ÄÇ
        """
        >>> splitter.split(paragraph)
        [
            'ÁæéÂõΩÊúÄÈ´òÊ≥ïÈô¢ÔºàËã±ËØ≠ÔºöSupreme Court of the United StatesÔºâÔºå‰∏ÄËà¨ÊòØÊåáÁæéÂõΩËÅîÈÇ¶ÊúÄÈ´òÊ≥ïÈô¢ÔºåÊòØÁæéÂõΩÊúÄÈ´òÁ∫ßÂà´ÁöÑËÅîÈÇ¶Ê≥ïÈô¢Ôºå‰∏∫ÁæéÂõΩ‰∏âÊùÉÁªßÊÄªÁªü„ÄÅÂõΩ‰ºöÂêéÊúÄ‰∏∫ÈáçË¶ÅÁöÑ‰∏ÄÁéØ„ÄÇ',
            'Ê†πÊçÆ1789Âπ¥„ÄäÁæéÂõΩÂÆ™Ê≥ïÁ¨¨‰∏âÊù°„ÄãÁöÑËßÑÂÆöÔºåÊúÄÈ´òÊ≥ïÈô¢ÂØπÊâÄÊúâËÅîÈÇ¶Ê≥ïÈô¢„ÄÅÂ∑ûÊ≥ïÈô¢ÂíåÊ∂âÂèäËÅîÈÇ¶Ê≥ïÂæãÈóÆÈ¢òÁöÑËØâËÆºÊ°à‰ª∂ÂÖ∑ÊúâÊúÄÁªàÔºàÂπ∂‰∏îÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊòØÊúâÊñüÈÖåÂÜ≥ÂÆöÊùÉÁöÑÔºâ‰∏äËØâÁÆ°ËæñÊùÉÔºå‰ª•ÂèäÂØπÂ∞èËåÉÂõ¥Ê°à‰ª∂ÁöÑÂÖ∑ÊúâÂàùÂÆ°ÁÆ°ËæñÊùÉ„ÄÇ',
            'Âú®ÁæéÂõΩÁöÑÊ≥ïÂæãÂà∂Â∫¶‰∏≠ÔºåÊúÄÈ´òÊ≥ïÈô¢ÈÄöÂ∏∏ÊòØÂåÖÊã¨„ÄäÁæéÂõΩÂÆ™Ê≥ï„ÄãÂú®ÂÜÖÁöÑËÅîÈÇ¶Ê≥ïÂæãÁöÑÊúÄÁªàËß£ÈáäËÄÖÔºå‰ΩÜ‰ªÖÂú®ÂÖ∑ÊúâÁÆ°ËæñÊùÉÁöÑÊ°à‰ª∂ËåÉÂõ¥ÂÜÖ„ÄÇ',
            'Ê≥ïÈô¢‰∏ç‰∫´ÊúâÂà§ÂÆöÊîøÊ≤ªÈóÆÈ¢òÁöÑÊùÉÂäõÔºõÊîøÊ≤ªÈóÆÈ¢òÁöÑÊâßÊ≥ïÊú∫ÂÖ≥ÊòØË°åÊîøÊú∫ÂÖ≥ÔºåËÄå‰∏çÊòØÊîøÂ∫úÁöÑÂè∏Ê≥ïÈÉ®Èó®„ÄÇ'
        ]
        ```
        
        ### Normalizer
        
        ```py
        # To normalize text
        >>> from icu_tokenizer import Normalizer
        >>> normalizer = Normalizer(lang='en', norm_puncts=True)
        
        >>> text = "ùëªùíâùíÜ ùíëùíìùíêùíÖùíñùíÑùíïùíî ùíöùíêùíñ ùíêùíìùíÖùíÜùíìùíÜùíÖ ùíòùíäùíçùíç ùíÉùíÜ ùíîùíâùíäùíëùíëùíÜùíÖ ùíÖùíäùíìùíÜùíÑùíïùíçùíö ùíáùíìùíêùíé ùë≤ùíêùíìùíÜùíÇ."
        >>> normalizer.normalize(text)
        "The products you ordered will be shipped directly from Korea."
        
        >>> text = "„Äê„ÄëÔºàÔºâ"
        >>> normalizer.normalize(text)
        "[]()"
        ```
        
        ### Tokenizer
        
        ```py
        >>> from icu_tokenizer import Tokenizer
        >>> tokenizer = Tokenizer(lang='th')
        
        >>> text = "‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏à‡∏µ‡∏ô ‡πÅ‡∏•‡∏∞‡∏≠‡∏≠‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏ï‡πà‡∏≠‡∏Ñ‡∏≥"
        >>> tokenizer.tokenize(text)
        ['‡∏†‡∏≤‡∏©‡∏≤', '‡πÑ‡∏ó‡∏¢', '‡πÄ‡∏õ‡πá‡∏ô', '‡∏†‡∏≤‡∏©‡∏≤', '‡∏ó‡∏µ‡πà', '‡∏°‡∏µ', '‡∏£‡∏∞‡∏î‡∏±‡∏ö', '‡πÄ‡∏™‡∏µ‡∏¢‡∏á', '‡∏Ç‡∏≠‡∏á', '‡∏Ñ‡∏≥', '‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô', '‡∏´‡∏£‡∏∑‡∏≠', '‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå', '‡πÄ‡∏ä‡πà‡∏ô', '‡πÄ‡∏î‡∏µ‡∏¢‡∏ß', '‡∏Å‡∏±‡∏ö', '‡∏†‡∏≤‡∏©‡∏≤', '‡∏à‡∏µ‡∏ô', '‡πÅ‡∏•‡∏∞', '‡∏≠‡∏≠‡∏Å', '‡πÄ‡∏™‡∏µ‡∏¢‡∏á', '‡πÅ‡∏¢‡∏Å', '‡∏Ñ‡∏≥', '‡∏ï‡πà‡∏≠', '‡∏Ñ‡∏≥']
        ```
        
Platform: UNKNOWN
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: License :: OSI Approved :: MIT License
Classifier: Topic :: Scientific/Engineering
Classifier: Topic :: Software Development
Classifier: Topic :: Software Development :: Libraries
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Software Development :: Localization
Description-Content-Type: text/markdown
